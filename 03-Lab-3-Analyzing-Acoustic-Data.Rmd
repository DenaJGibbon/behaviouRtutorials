# Computer Lab 3. Analyzing Acoustic Data {-}


```{r echo=FALSE,fig.align='center',warning=FALSE,message=FALSE}
library(behaviouR)
library(tuneR)
library(seewave)
library(stringr)
library(plyr)

#  pdf('spectrograms.pdf',width=10)
#  par(mfrow=c(2,2))
#
# SpectrogramSingle(sound.file ="SoundscapeRecordings/Borneo_Morning_01.wav",
#                   min.freq = 0,max.freq=22000,
#                   Colors = 'Colors',downsample = FALSE)
#
# SpectrogramSingle(sound.file ="SoundscapeRecordings/Borneo_Night_01.wav",
#                   min.freq = 0,max.freq=22000,
#                   Colors = 'Colors',downsample = FALSE)
#
# SpectrogramSingle(sound.file ="SoundscapeRecordings/Sulawesi_Morning_01.wav",
#                   min.freq = 0,max.freq=22000,
#                   Colors = 'Colors',downsample = FALSE)
#
# SpectrogramSingle(sound.file ="SoundscapeRecordings/Sulawesi_Night_01.wav",
#                   min.freq = 0,max.freq=22000,
#                   Colors = 'Colors',downsample = FALSE)
#
#
#
# graphics.off()

Fig3a <- magick::image_read('/Users/denasmacbook/behaviouR-package/behaviouR tutorial script/images/Lab3Figure.pdf')
Fig3a <-magick::image_ggplot(Fig3a)

Fig3a
# Fig3a <- magick::image_read('/Users/denasmacbook/behaviouR-package/behaviouR tutorial script/spectrograms.pdf')
# Fig3a <-magick::image_ggplot(Fig3a)




# # Then we extract the features, which in this case are the MFCCs.
# source("Functions/MFCCFunctionSite.R")
# SoundscapeFeatureDataframe <- 
#   MFCCFunctionSite(input.dir = "SoundscapeRecordings",min.freq = 200,max.freq=10000)
# 
# 
# SoundscapeFeatureDataframe$Class <-revalue(SoundscapeFeatureDataframe$Class,c(
#   Borneo_Morning='Borneo \n Morning',
#   Borneo_Night='Borneo \n Night',
#   Sulawesi_Morning='Sulawesi \n Morning',
#   Sulawesi_Night='Sulawesi \n Night'))
# 
# # Now we visualize our results
# pca_res <- prcomp(SoundscapeFeatureDataframe[,-c(1)], scale. = TRUE)
# Fig3b <- autoplot(pca_res, data = SoundscapeFeatureDataframe, 
#          colour = 'Class')+theme_bw()+scale_color_manual(values=matlab::jet.colors(4))
# 
# 
# cowplot::plot_grid(Fig3a,Fig3b,labels = c('A','B'),scale = c(1.15,1))



```

*Background*   
In this lab you will become familiar with the ways that we analyze acoustic data. You will start by analyzing focal recordings and soundscapes from Southeast Asia, and you will then upload the data you collected for Field Lab 3 and do some prelminary analysis.

*Goals of the exercises*    
The main goal(s) of today's lab are to:  
1) Create your own spectrograms  
2) Learn how we use PCA to visualize differences in acoustic data   
3) Upload and visualize your own data


*Getting started*    
First we need to load the relevant packages for our data analysis. Packages contain all the functions that are needed for data analysis. 
First we load the required libraries
```{r, echo=TRUE, warning=FALSE,message=FALSE}

library(behaviouR)
```

We need to do some data prep to work through the examples
```{r, eval=F, echo=T}

# First you can check where your working directory is; this is where R will store the files locally
getwd()

# Now we will create a file at this location called 'FocalRecordings'
dir.create(file.path('FocalRecordings'), showWarnings = FALSE)

# Now we will load the sound files that were in the R package
githubURL <-'https://github.com/DenaJGibbon/behaviouRdata/raw/master/data/FocalRecordings.rda'
FocalRecordings <- get(load(url(githubURL)))

# Let's check the structure
head(FocalRecordings)

# Now we will save the recordings to the new folder you created
for(a in 1:length(FocalRecordings)){
  FileName <- FocalRecordings[[a]][1][[1]]
  WaveFile <- FocalRecordings[[a]][2][[1]]
  tuneR::writeWave(WaveFile, paste("FocalRecordings/",FileName,sep=''))
}
```

## Part 1. Loading a sound file and making a spectrogram {-}
First we want to check which files are in the folder
There are five gibbon female recordings and five great argus recordings. 
```{r, echo=TRUE}

list.files("FocalRecordings")
```


Now let's read in one of the files for further investigation. We do that with the function 'readWave'. The .wav stands for 'Waveform Audio File' and is a common format that scientists use to save their sound files.

```{r, echo=TRUE}

GibbonWaveFile <- tuneR::readWave("FocalRecordings/FemaleGibbon_1.wav")
```

Now if we run the object it will return some information
```{r, echo=TRUE}
GibbonWaveFile
```

What we see is that the soundfile is ~13 seconds long, was recorded at a sampling rate of 44100 samples per second and has a total of 572054 samples. We can check if that makes sense using the following equation (duration* sampling rate).
```{r, echo=TRUE}

seewave::duration(GibbonWaveFile)* GibbonWaveFile@samp.rate
```

Now we can plot the waveform from our sound file using the following code:
```{r, echo=TRUE}

seewave::oscillo(GibbonWaveFile)
```

We can also zoom in so that we can actually see the shape of the wave. 
```{r, echo=TRUE}

seewave::oscillo(GibbonWaveFile, from=0.1, to=0.2)
```

And if we zoom in again we see the wave shape even more. 
```{r, echo=TRUE}

seewave::oscillo(GibbonWaveFile, from=0.15, to=0.2)
```

As we learned in class this week, a common way for scientists to study animal sounds is through the use of spectrograms. Here we will plot a spectrogram of a single gibbon call.

```{r, echo=TRUE, results='hide'}

SpectrogramSingle(sound.file ="FocalRecordings/FemaleGibbon_1.wav")
```

There are many different things you can change when creating a spectrogram. In the above version of the plot there is a lot of space above the gibbon calls, so we will change the frequency range to better visualize the gibbons.

```{r, echo=TRUE, results='hide'}

SpectrogramSingle(sound.file ="FocalRecordings/FemaleGibbon_1.wav",
                  min.freq = 500,max.freq=2500)
                  
```

In the code we are using there is also the option to change the color of the spectrogram adding the following code:

```{r, echo=TRUE, results='hide'}
SpectrogramSingle(sound.file ="FocalRecordings/FemaleGibbon_1.wav",
                  min.freq = 500,max.freq=2500,
                  Colors = 'Colors')
```

NOTE: Changing the colors doesn't change how we read the spectrograms,and different people have different preferences.

Now we can plot all of the spectrograms in our FocalRecordings file at once.

NOTE: Use the navigation buttons on the plots tab to move back and forth between the different spectrograms.
```{r, echo=TRUE, warning=FALSE, message=FALSE, results='hide'}
# We can tell R to print the spectrograms 2x2 using the code below
par(mfrow=c(2,2))

# This is the function to create the spectrograms
  SpectrogramFunction(input.dir = "FocalRecordings",min.freq = 500,max.freq=2500)
```

*Question 1.* What differences do you notice about gibbon and great argus spectrograms? 

Now that you are doing looking at the spectrograms we will clear the space. You can always re-run the code to look at the again.
```{r, eval=FALSE}

graphics.off()
```

## Part 2. Visualizing differences in gibbon and great argus calls {-}
A major question in many different types of behavioral studies is whether there are differences between groups. This is also the case for acoustic data, where researchers may be interested if there are differences between sexes, populations, individuals, or vocalizations emitted in different contexts. Here we will work through a simple example where we will investigate differences in gibbon and argus calls.  

The first step that we need to do is called feature extraction. There are many different ways that scientists do this, but the overarching idea is that sound data contains too much redundant information to be used on a model. Computers just don't have enough power to crunch all those numbers, so we need to identify a meaningful set of features that is smaller than using the while waveform. This is also the case for image processing.  

We are going to use a feature extraction method called 'Mel-frequency cepstral coefficients'. I will not expect you to know any more about them, apart from the fact they are very useful for human speech and bioaccoustics applications.   

Below is the function to extract features from our focal recordings
```{r, echo=TRUE, warning=FALSE, message=FALSE, results='hide'}

FeatureDataframe <- MFCCFunction(input.dir = "FocalRecordings")
```

In this new dataframe each row represents one gibbon or great argus call. Let's check the dimensions of this new dataframe.
```{r, echo=TRUE}

dim(FeatureDataframe)
```

This shows us that for each of the 10 calls there are 178 features. This is in contrast to a sound file. Let's check again to remind ourselves.
```{r, echo=TRUE}

GibbonWaveFile
```

This sound file is comprised of 572054 numbers. Now let's check the first row of our new dataframe which contains the new features for the same gibbon call.

```{r, echo=TRUE}
# This isolates the first row.
FeatureDataframe[1,] 

# This tells us how many features we have.
ncol(FeatureDataframe[1,]) 
```

OK, now we are ready to do some plotting. If you remember from our earlier lab, the data structure we have here is multivariate, as each call has multiple values associated with it. So we will use the same approach (principal component analysis) to visualize our gibbon and great argus calls.   

First we run the principal component analysis
```{r, echo=TRUE}

pca_res <- prcomp(FeatureDataframe[,-c(1)], scale. = TRUE)
```

Now we visualize our results
```{r, echo=TRUE,warning=FALSE, message=FALSE, results='hide'}

ggplot2::autoplot(pca_res, data = FeatureDataframe, 
         colour = 'Class')
```

What we see in this plot is strong clustering, with the points from each class of calls more close to the other points than those of different calls.

*Question 2.* How do we interpret the clustering in this PCA plot?


## Part 3. Soundscapes {-}
Now we will do something similar to our investigation of the focal recordings, but using a soundscape example. Here we have 20-sec recordings taken from two different locations (Borneo and Sulawesi) and two different times (06:00 and 18:00). Let's check what is in the folder.

```{r, echo=TRUE}

list.files("SoundscapeRecordings")
```

Now we will create spectrograms for each recording
```{r, echo=TRUE, results='hide'}
par(mfrow=c(2,2))
SpectrogramFunctionSite(input.dir = "SoundscapeRecordings",
                    min.freq = 0,max.freq=20000)
```

Now that you are doing looking at the figure we will clear the space. You can always re-run the code to look at the again.
```{r}
graphics.off()
```

Now just as above we will do feature extraction of our soundscape recordings. Thiswill convert our dataset into a smaller, much more manageable size. 

```{r, echo=TRUE, warning=FALSE, message=FALSE, results='hide'}
SoundscapeFeatureDataframe <- 
  MFCCFunctionSite(input.dir = "SoundscapeRecordings")
```

Check the resulting structure of the dataframe to make sure it looks OK.
```{r, echo=TRUE}
dim(SoundscapeFeatureDataframe)
```

Now we visualize our results
```{r, warning=FALSE, message=FALSE, results='hide'}

pca_res <- prcomp(SoundscapeFeatureDataframe[,-c(1)], scale. = TRUE)
ggplot2::autoplot(pca_res, data = SoundscapeFeatureDataframe, 
         colour = 'Class')
```

It looks like the Borneo_Night sampling period was much different from the rest! Let's look at a single spectrogram from each sampling location and time.
```{r, echo=TRUE, results='hide'}
par(mfrow=c(2,2))
SpectrogramSingle(sound.file ="SoundscapeRecordings/Borneo_Night_01.wav",
                  min.freq = 0,max.freq=22000,
                  Colors = 'Colors',downsample = FALSE)

SpectrogramSingle(sound.file ="SoundscapeRecordings/Borneo_Morning_01.wav",
                  min.freq = 0,max.freq=22000,
                  Colors = 'Colors',downsample = FALSE)

SpectrogramSingle(sound.file ="SoundscapeRecordings/Sulawesi_Night_01.wav",
                  min.freq = 0,max.freq=22000,
                  Colors = 'Colors',downsample = FALSE)

SpectrogramSingle(sound.file ="SoundscapeRecordings/Sulawesi_Morning_01.wav",
                  min.freq = 0,max.freq=22000,
                  Colors = 'Colors',downsample = FALSE)
```

*Question 3*. You can listen to example sound files on the Canvas page. Between looking at the spectrograms and listening to the sound files what do you think are the main differences between the different locations and times?

## Part 4. Now it is time to analyze the data you collected for this week's field lab. {-}
Upload your sound files into either the 'MyFocalRecordings' folder or the 'MySoundscapeRecordings'folder and run the code below.

NOTE: Make sure to delete the existing files before you add your own!

### Part 4a. Your focal recordings
First lets visualize the spectrograms. You may want to change the frequency settings depending on the frequency range of your focal animals.
```{r, echo=TRUE, warning=FALSE, message=FALSE, results='hide'}
# We can tell R to print the spectrograms 2x2 using the code below
par(mfrow=c(2,2))

# This is the function to create the spectrograms
SpectrogramFunction(input.dir = "MyFocalRecordings",min.freq = 200,max.freq=2000)
```


Here is the function to extract the features. Again, based on the frequency range of your focal recordings you may want to change the frequency settings.
```{r, echo=TRUE, results='hide'}

MyFeatureDataFrame <- MFCCFunction(input.dir = "MyFocalRecordings",min.freq = 200,max.freq=2000)
```

Then we run the principal component analysis
```{r, echo=TRUE, results='hide'}

pca_res <- prcomp(MyFeatureDataFrame[,-c(1)], scale. = TRUE)
```

Now we visualize our results
```{r, echo=TRUE, results='hide'}

ggplot2::autoplot(pca_res, data = MyFeatureDataFrame,
         colour = 'Class')
```

### Part 4b. Soundscape recordings
First lets visualize the spectrograms. You probably don't want to change the frequency settings for this.
```{r, echo=TRUE, warning=FALSE, message=FALSE, results='hide'}
# We can tell R to print the spectrograms 2x2 using the code below
par(mfrow=c(2,2))

# This is the function to create the spectrograms
SpectrogramFunction(input.dir = "MySoundscapeRecordings",min.freq = 200,max.freq=10000)
```

Then we extract the features, which in this case are the MFCCs.
```{r, echo=TRUE, results='hide',warning=FALSE}

MySoundscapeFeatureDataframe <- 
  MFCCFunctionSite(input.dir = "MySoundscapeRecordings",min.freq = 200,max.freq=10000)
```

Check the resulting structure of the dataframe to make sure it looks OK.
```{r, echo=TRUE, results='hide'}
dim(MySoundscapeFeatureDataframe)
```

Now we visualize our results
```{r, echo=TRUE, results='hide'}

pca_res <- prcomp(MySoundscapeFeatureDataframe[,-c(1)], scale. = TRUE)
ggplot2::autoplot(pca_res, data = MySoundscapeFeatureDataframe, 
         colour = 'Class')

```

*Question 4*. Do you see evidence of clustering in either your focal recordings or soundscape recordings? 
